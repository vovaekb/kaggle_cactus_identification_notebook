{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "simple_vgg_like_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vovaekb/kaggle_cactus_identification_notebook/blob/master/simple_vgg_like_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "226ff26188d6e4d6e05bd316332a835770b19cf3",
        "id": "x7kN9REn_KCD",
        "colab_type": "text"
      },
      "source": [
        "This kernel is a fork of the [Aerial Cactus - Simple CNN](https://www.kaggle.com/masonblier/aerial-cactus-simple-cnn)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "eDvLnVr-_KCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc\n",
        "import glob\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import imageio as im\n",
        "import keras\n",
        "from keras import models\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout, Flatten, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.optimizers import adam\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "print(os.listdir(\"../input/\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "275566e3f7271baf633dc59c0d6a54c64392a8ba",
        "id": "w9fBuWd2_KCV",
        "colab_type": "text"
      },
      "source": [
        "Here I write the function for loading and preprocessing the image data. There's only one target category in this dataset so I show the first 8 images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "scrolled": true,
        "id": "oL1_cnKK_KCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load images dataset\n",
        "def loadImagesData(glob_path):\n",
        "    images = []\n",
        "    names = []\n",
        "    for img_path in glob.glob(glob_path):\n",
        "        # load/resize images with cv2\n",
        "        names.append(os.path.basename(img_path))\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        images.append(img) # already 32x32\n",
        "    return (images,names)\n",
        "# map of training label to list of images\n",
        "trainData = {}\n",
        "namesData = {}\n",
        "for label in os.listdir('../input/train/'):\n",
        "    (images,names) = loadImagesData(f\"../input/train/{label}/*.jpg\")\n",
        "    print(f\"../input/train/{label}/*.jpg\")\n",
        "    trainData[label] = images\n",
        "    namesData[label] = names\n",
        "print(\"train labels:\", \",\".join(trainData.keys()))\n",
        "print(len(trainData['train']))\n",
        "# show some data\n",
        "plt.figure(figsize=(4,2))\n",
        "columns = 4\n",
        "for i in range(0,8):\n",
        "    plt.subplot(8 / columns + 1, columns, i + 1)\n",
        "    plt.imshow(trainData['train'][i])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "1360aa7f6cca60ddff80a1e935088c38485c2f0f",
        "id": "ay7v3QgC_KCj",
        "colab_type": "text"
      },
      "source": [
        "Checking out the train.csv data, use value_counts to check relative number of 0 and 1 has_cactus values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "6c11cdeeaf0a6a2b1473e522ffd9e30729dd740c",
        "id": "U19MrTSW_KCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_meta = pd.read_csv('../input/train.csv')\n",
        "print(train_meta.shape)\n",
        "print(train_meta.has_cactus.value_counts())\n",
        "# lookup table of name to has_cactus\n",
        "lookupY = {}\n",
        "for i in range(0,len(train_meta)):\n",
        "    row = train_meta.iloc[i,:]\n",
        "    lookupY[row.id] = row.has_cactus\n",
        "train_meta.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "8eb9797aed9ae27681c083537eb50661cff8948f",
        "id": "5BPi6brN_KCp",
        "colab_type": "text"
      },
      "source": [
        "Build a dataframe of all the x and y data. I then build a more even dataset of 50% 0 and 1 to help the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "ecae93b2162bc219dbd0b76a5f367fdee2b9fb8d",
        "id": "5_u5wveA_KCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build x/y dataset\n",
        "trainList = []\n",
        "maxCount = 4364 # number of has_cactus = 0\n",
        "counts = {'0':0,'1':0}\n",
        "for (i,image) in enumerate(trainData['train']):\n",
        "    label = lookupY[namesData['train'][i]]\n",
        "    counts[str(label)] = 1 + counts[str(label)]\n",
        "    if counts[str(label)] < maxCount:\n",
        "        trainList.append({\n",
        "            'label': label,\n",
        "            'data': image\n",
        "        })\n",
        "# shuffle dataset\n",
        "random.shuffle(trainList)\n",
        "# dataframe and display\n",
        "train_df = pd.DataFrame(trainList)\n",
        "gc.collect()\n",
        "print(train_df.shape)\n",
        "print(train_df.label.value_counts())\n",
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "ab4891b48b45e404d5dfb985ea894b7f5280f8ad",
        "id": "8ndDMs-V_KCv",
        "colab_type": "text"
      },
      "source": [
        "Encode x data as numpy stack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "ddc5bfbab4fa046658c2f9b6288085a636412146",
        "id": "pe3ugJok_KCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encode training data\n",
        "data_stack = np.stack(train_df['data'].values)\n",
        "dfloats = data_stack.astype(np.float)\n",
        "all_x = np.multiply(dfloats, 1.0 / 255.0) # np.array(train_df['data'].values, dtype=np.float) / 255.0\n",
        "print(all_x.shape)\n",
        "print(type(all_x))\n",
        "all_x[0,0,0,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "2267d8c7c560f9655aaebf039eb2bfe2ff7a6136",
        "id": "pWn9fT_p_KC5",
        "colab_type": "text"
      },
      "source": [
        "Since we use binary_crossentropy, the y category data just needs to be made as floats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "660e33349313a629ab621c2132abcdbc7557244d",
        "id": "laIOf6eL_KC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_y = np.array(train_df.label).astype(np.float)\n",
        "all_y[0:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e5a78f18d0425de973fad2a347a8a622bf878ee8",
        "id": "PH4r0iTK_KDC",
        "colab_type": "text"
      },
      "source": [
        "Make the training/validation split to measure training accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "e34414c0b895198fe3d9b3f7ee77a7a9090a7f92",
        "id": "iXQjlwvZ_KDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split test/training data\n",
        "train_x,test_x,train_y,test_y=train_test_split(all_x,all_y,test_size=0.2,random_state=7)\n",
        "print(train_x.shape,test_x.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "0e128219f908c31b41ce203b68f0396e36abbfba",
        "id": "iIWBvGyG_KDN",
        "colab_type": "text"
      },
      "source": [
        "Here I define the data augmenter. I use x,y and rotation as the images were taken from aerial and thus can vary in these ways."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "b6e73b078d2f421c7b67df2348436aa9a027b713",
        "id": "zAFWTKfH_KDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x,y and rotation data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    rotation_range=60,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    zoom_range=0.2, # zoom images\n",
        "    horizontal_flip=True,  # randomly flip images\n",
        "    vertical_flip=True)  # randomly flip images\n",
        "datagen.fit(train_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "6124ab0962b6041f6799c7ce95628ce0ea122534",
        "id": "uiccN_2h_KDe",
        "colab_type": "text"
      },
      "source": [
        "> Starting with the simple stacked 3x3 conv net from Towards Data Science\n",
        "\n",
        "This model is left from the original kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "08ffa8e90df9abd8729c971eaf2320d6b58940bf",
        "id": "lrqy_bI6_KDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the network\n",
        "num_filters = 8\n",
        "input_shape = train_x.shape[1:]\n",
        "output_shape = 1\n",
        "# model\n",
        "m = Sequential()\n",
        "def tdsNet(m):\n",
        "    m.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
        "    m.add(Conv2D(16, kernel_size=3, activation='relu'))\n",
        "    m.add(Flatten())\n",
        "    m.add(Dropout(0.5))\n",
        "    m.add(Dense(units = output_shape, activation='sigmoid'))\n",
        "tdsNet(m)\n",
        "# compile adam with decay and use binary_crossentropy for single category dataset\n",
        "m.compile(optimizer = 'nadam',\n",
        "          loss = 'binary_crossentropy', \n",
        "          metrics = ['accuracy'])\n",
        "# show summary\n",
        "m.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "a0748e00120312c672304a12abdfea5797ca611e",
        "id": "_hsrlDhM_KDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train model\n",
        "batch_size = 32\n",
        "history = m.fit_generator(datagen.flow(train_x, train_y,\n",
        "                          batch_size=batch_size),\n",
        "                          steps_per_epoch= (train_x.shape[0] // batch_size),\n",
        "                          epochs = 4,\n",
        "                          validation_data=(test_x, test_y),\n",
        "                          workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f3a2c0c388945451c28f0c5ddb9abe883e3eea42",
        "id": "ARBRgK7g_KDq",
        "colab_type": "text"
      },
      "source": [
        "After series of experiments I end up with a VGG like CNN network (Adrian Rosebrock explains this architecture in his book \"Deep Learning for Computer Vision with Python - Practitioner Bundle\" in chapter 7). This model includes 3 pairs of Conv2D layers with 3x3 kernels (32, 64 and 128 filters in each pair of Conv2D layers accordingly). Second pair of Conv2D layers is followed by MaxPooling2D layer with size 2x2, second MaxPooling layer is included between the last pair of Conv2D layers and Fully Connected layer. Large depth of the CNN network allows to learn richer and more distinctive features on images while BatchNormalization and Dropout layers reduce the overfit when training the network.\n",
        "\n",
        "Architecture scheme:\n",
        "CONV(32) => RELU => CONV(32) => RELU => DROPOUT =>\n",
        "CONV(64) => RELU => DROPOUT => CONV(64) => RELU => POOL => DROPOUT => \n",
        "CONV (128) => RELU =>  DROPOUT =>\n",
        "CONV (128) => RELU => POOL => DROPOUT => FC\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "669261343e9a1a101cc2d9f1fc381687561efe99",
        "id": "C7MpSMbe_KDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the network\n",
        "num_filters = 8\n",
        "input_shape = train_x.shape[1:]\n",
        "output_shape = 1\n",
        "# model\n",
        "m = Sequential()\n",
        "\n",
        "def cnnNet(m):\n",
        "    # First pair of Conv2D layers\n",
        "    m.add(Conv2D(32, kernel_size=3, input_shape=input_shape))\n",
        "    m.add(BatchNormalization())\n",
        "    m.add(Activation(\"relu\"))\n",
        "    \n",
        "    m.add(Conv2D(32, kernel_size=3))\n",
        "    m.add(BatchNormalization())\n",
        "    m.add(Activation(\"relu\"))\n",
        "    m.add(Dropout(0.25))\n",
        "    \n",
        "    # Second pair of Conv2D layers\n",
        "    m.add(Conv2D(64, kernel_size=3))\n",
        "    m.add(BatchNormalization())\n",
        "    m.add(Activation(\"relu\"))\n",
        "    m.add(Dropout(0.25))\n",
        "    \n",
        "    m.add(Conv2D(64, kernel_size=3))\n",
        "    m.add(BatchNormalization())\n",
        "    m.add(Activation(\"relu\"))\n",
        "    m.add(MaxPooling2D(2,2))\n",
        "    m.add(Dropout(0.25))\n",
        "    \n",
        "    # Third pair of Conv2D layers\n",
        "    m.add(Conv2D(128, kernel_size=3))\n",
        "    m.add(BatchNormalization())\n",
        "    m.add(Activation(\"relu\"))\n",
        "    m.add(Dropout(0.25))\n",
        "    \n",
        "    m.add(Conv2D(128, kernel_size=3))\n",
        "    m.add(BatchNormalization())\n",
        "    m.add(Activation(\"relu\"))\n",
        "    \n",
        "    m.add(MaxPooling2D(2,2))\n",
        "    m.add(Dropout(0.25))\n",
        "    \n",
        "    m.add(Flatten())\n",
        "    \n",
        "    m.add(Dense(64, activation='relu'))\n",
        "    m.add(BatchNormalization())\n",
        "    m.add(Dropout(0.5))\n",
        "    m.add(Dense(units = output_shape, activation='sigmoid'))\n",
        "\n",
        "    \n",
        "cnnNet(m)\n",
        "# compile adam with decay and use binary_crossentropy for single category dataset\n",
        "m.compile(optimizer = 'nadam',\n",
        "          loss = 'binary_crossentropy', \n",
        "          metrics = ['accuracy'])\n",
        "# show summary\n",
        "m.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "a77f86d9441e66c9c560ef0cfa34a98e8a489d91",
        "scrolled": false,
        "id": "fP2vkX3u_KDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### train model\n",
        "batch_size = 64\n",
        "history = m.fit_generator(datagen.flow(train_x, train_y,\n",
        "                          batch_size=batch_size),\n",
        "                          steps_per_epoch= (train_x.shape[0] // batch_size),\n",
        "                          epochs = 95,\n",
        "                          validation_data=(test_x, test_y),\n",
        "                          workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "92ead9ae8f992074a70cef3d3e25a49225cfe7f0",
        "id": "B7EPEU9Y_KD2",
        "colab_type": "text"
      },
      "source": [
        "Finish training on the rest of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "98489d81e7a4bd64eed44bbf7152606aa5b165ef",
        "scrolled": true,
        "id": "vjTkGXmy_KD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build complete x/y dataset\n",
        "trainList = []\n",
        "for (i,image) in enumerate(trainData['train']):\n",
        "    label = lookupY[namesData['train'][i]]\n",
        "    trainList.append({\n",
        "        'label': label,\n",
        "        'data': image\n",
        "    })\n",
        "# shuffle dataset\n",
        "random.shuffle(trainList)\n",
        "# dataframe and display\n",
        "train_df = pd.DataFrame(trainList)\n",
        "gc.collect()\n",
        "# encode training data\n",
        "data_stack = np.stack(train_df['data'].values)\n",
        "dfloats = data_stack.astype(np.float)\n",
        "all_x = np.multiply(dfloats, 1.0 / 255.0)\n",
        "all_x.shape\n",
        "all_y = np.array(train_df.label).astype(np.float)\n",
        "# split test/training data\n",
        "train_x,test_x,train_y,test_y=train_test_split(all_x,all_y,test_size=0.2,random_state=7)\n",
        "print(train_x.shape,test_x.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "35749abb8a47c8ef8412e49d6038024ce065edd9",
        "id": "UyGXVfyj_KEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###### continue training model\n",
        "batch_size = 128 # 64\n",
        "history = m.fit_generator(datagen.flow(train_x, train_y,\n",
        "                          batch_size=batch_size),\n",
        "                          steps_per_epoch= (train_x.shape[0] // batch_size),\n",
        "                          epochs = 95,\n",
        "                          validation_data=(test_x, test_y),\n",
        "                          workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "49b07ad3c475495d8cff945f4e453d45334d9a5a",
        "id": "2j-GXRA-_KEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check sample submission format\n",
        "pd.read_csv('../input/sample_submission.csv').head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "364fea532dbd56b290b316f8485c21d0f29cbf49",
        "id": "awMvOy53_KEM",
        "colab_type": "text"
      },
      "source": [
        "Output predictions file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "4e45847a3cdfb43c464b5e6c1849bf5e3b5f7aaf",
        "id": "swlCLfkl_KEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# output predicted submission csv\n",
        "(test_images, test_names) = loadImagesData(f\"../input/test/test/*.jpg\")\n",
        "data_stack = np.stack(test_images)\n",
        "dfloats = data_stack.astype(np.float32)\n",
        "unknown_x = np.multiply(dfloats, 1.0 / 255.0)\n",
        "# predict\n",
        "predicted = np.ravel(m.predict(unknown_x))\n",
        "submission_df = pd.DataFrame({'id':test_names,'has_cactus':predicted})\n",
        "submission_df.to_csv('submission.csv', index=False)\n",
        "len(submission_df)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}